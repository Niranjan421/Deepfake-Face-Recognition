{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIKe-kZgm0rx"
      },
      "source": [
        "**IMPORTING LIBRARIES**\n",
        "\n",
        "Importing various libraries and packages that we would be needing in our project."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "WMMhThYomx1H"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7ux6Gx6oNSZ"
      },
      "source": [
        "**DATASET READING AND PREPROCESSING**\n",
        "\n",
        "Dataset contains real and fake videos which will be used for training the model.\n",
        "\n",
        "Data preprocessing will be in following steps :\n",
        "    \n",
        "    1. Split the videos into frames\n",
        "    2. Detect faces in the frames\n",
        "    3. Crop the detected faces to focus the model on facial features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed video F:\\Niranjan\\abc\\real\\01__hugging_happy.mp4\n",
            "Processed video F:\\Niranjan\\abc\\real\\01__outside_talking_still_laughing.mp4\n",
            "Processed video F:\\Niranjan\\abc\\real\\01__podium_speech_happy.mp4\n",
            "Processed video F:\\Niranjan\\abc\\real\\01__talking_against_wall.mp4\n",
            "Processed video F:\\Niranjan\\abc\\real\\01__walk_down_hall_angry.mp4\n",
            "Processed video F:\\Niranjan\\abc\\fake\\01_02__outside_talking_still_laughing__YVGY8LOK.mp4\n",
            "Processed video F:\\Niranjan\\abc\\fake\\01_02__walk_down_hall_angry__YVGY8LOK.mp4\n",
            "Processed video F:\\Niranjan\\abc\\fake\\01_03__hugging_happy__ISF9SP4G.mp4\n",
            "Processed video F:\\Niranjan\\abc\\fake\\01_03__podium_speech_happy__480LQD1C.mp4\n",
            "Processed video F:\\Niranjan\\abc\\fake\\01_03__talking_against_wall__JZUXXFRB.mp4\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import os\n",
        "import dlib\n",
        "\n",
        "# MENTION THE DIRECTORIES FOR REAL AND FAKE VIDEOS\n",
        "rvideos_dir = r\"F:\\Niranjan\\abc\\real\"    # REAL VIDEOS\n",
        "fvideos_dir = r\"F:\\Niranjan\\abc\\fake\"    # FAKE VIDEOS\n",
        "\n",
        "# DIRECTORIES WHERE PREPROCESSED FRAMES FROM REAL AND FAKE VIDEOS WILL BE SAVED\n",
        "output_real_dir = r\"F:\\Niranjan\\ansh\\real\"\n",
        "output_fake_dir = r\"F:\\Niranjan\\ansh\\fake\"\n",
        "\n",
        "# ENSURE OUTPUT DIRECTORIES EXIST\n",
        "os.makedirs(output_real_dir, exist_ok=True)\n",
        "os.makedirs(output_fake_dir, exist_ok=True)\n",
        "\n",
        "# LOAD PRETRAINED FACE DETECTOR\n",
        "detector = dlib.get_frontal_face_detector()\n",
        "\n",
        "# FUNCTION TO EXTRACT FRAMES AND PROCESS THEM (FACE DETECTION + CROPPING)\n",
        "def pro_video(video_path, output_dir):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    if not cap.isOpened():\n",
        "        print(f\"Error: Could not open video file {video_path}\")\n",
        "        return\n",
        "\n",
        "    frame_count = 0\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        # Detect Faces\n",
        "        faces = detector(frame, 1)\n",
        "\n",
        "        # If faces are detected then crop and save them\n",
        "        for i, face in enumerate(faces):\n",
        "            x = face.left()\n",
        "            y = face.top()\n",
        "            w = face.width()\n",
        "            h = face.height()\n",
        "\n",
        "            cropped_face = frame[y:y+h, x:x+w]\n",
        "\n",
        "            # Saving the cropped face as an image\n",
        "            output_path = os.path.join(output_dir, f\"{os.path.basename(video_path)}_frame_{frame_count}_face_{i}.jpg\")\n",
        "            cv2.imwrite(output_path, cropped_face)\n",
        "\n",
        "        frame_count += 1\n",
        "\n",
        "    cap.release()\n",
        "    print(f\"Processed video {video_path}\")\n",
        "\n",
        "# PROCESS ALL REAL VIDEOS\n",
        "for filename in os.listdir(rvideos_dir):\n",
        "    if filename.lower().endswith(\".mp4\"):\n",
        "        video_path = os.path.join(rvideos_dir, filename)\n",
        "        pro_video(video_path, output_real_dir)\n",
        "\n",
        "# PROCESS ALL FAKE VIDEOS\n",
        "for filename in os.listdir(fvideos_dir):\n",
        "    if filename.lower().endswith(\".mp4\"):\n",
        "        video_path = os.path.join(fvideos_dir, filename)\n",
        "        pro_video(video_path, output_fake_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JgrNl7LM3DC9"
      },
      "source": [
        "**MODEL TRAINING**\n",
        "\n",
        "For training the model we will be using ResNet50\n",
        "\n",
        "ResNet50 is basically a pre-tarined model used for image classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "K9W_2kOC31jU"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "-xODOmgEJ1k2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 6897 images belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "# Directories for training data (containing frames from real and fake videos)\n",
        "train_dir = \"F:\\\\Niranjan\\\\ansh\"\n",
        "\n",
        "img_size = (224, 224)  # ResNet50 expects this size\n",
        "batch_size = 32\n",
        "num_classes = 2   # Real and Fake\n",
        "\n",
        "# Data augmentation and normalization\n",
        "train_datagen = ImageDataGenerator(rescale = 1./255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True)\n",
        "\n",
        "# Training generator\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size = img_size,\n",
        "    batch_size = batch_size,\n",
        "    class_mode = 'categorical'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "PCuzCHqlLi52"
      },
      "outputs": [],
      "source": [
        "# Load ResNet50 with pre-trained ImageNet\n",
        "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Add custom layers on top of the base model\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(256, activation='relu')(x)  # Fully connected layer\n",
        "predictions = Dense(num_classes, activation='softmax')(x)  # Output layer for classification (True/False)\n",
        "\n",
        "# Create the final model\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# Freeze ResNet50 base model layers\n",
        "for layer in base_model.layers:\n",
        "  layer.trainable = False\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Nrlu0e7IMxvG"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\python\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m342s\u001b[0m 2s/step - accuracy: 0.5030 - loss: 0.7622\n",
            "Epoch 2/10\n",
            "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m327s\u001b[0m 2s/step - accuracy: 0.5179 - loss: 0.7021\n",
            "Epoch 3/10\n",
            "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m327s\u001b[0m 2s/step - accuracy: 0.5239 - loss: 0.7039\n",
            "Epoch 4/10\n",
            "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m373s\u001b[0m 2s/step - accuracy: 0.5254 - loss: 0.6954\n",
            "Epoch 5/10\n",
            "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m429s\u001b[0m 2s/step - accuracy: 0.5419 - loss: 0.6904\n",
            "Epoch 6/10\n",
            "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m372s\u001b[0m 2s/step - accuracy: 0.5183 - loss: 0.6908\n",
            "Epoch 7/10\n",
            "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m376s\u001b[0m 2s/step - accuracy: 0.5355 - loss: 0.6885\n",
            "Epoch 8/10\n",
            "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m363s\u001b[0m 2s/step - accuracy: 0.5443 - loss: 0.6872\n",
            "Epoch 9/10\n",
            "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m395s\u001b[0m 2s/step - accuracy: 0.5515 - loss: 0.6882\n",
            "Epoch 10/10\n",
            "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m364s\u001b[0m 2s/step - accuracy: 0.5310 - loss: 0.6881\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x111dcf712b0>"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# TRAIN THE MODEL\n",
        "\n",
        "epochs = 10\n",
        "model.fit(\n",
        "    train_generator,\n",
        "    epochs = epochs\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "b4wey3GlNATL"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ],
      "source": [
        "# SAVE THE MODEL\n",
        "model.save('F:\\\\Niranjan\\\\model.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUQtDzyGNPlT"
      },
      "source": [
        "**MODEL EVALUATION**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "9t9FzHiqNPLP"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m406s\u001b[0m 2s/step - accuracy: 0.6136 - loss: 0.6846\n",
            "Train Loss: 0.6846863627433777\n",
            "Train Accuracy: 0.6122952103614807\n"
          ]
        }
      ],
      "source": [
        "# Import the load_model function from keras.models\n",
        "from keras.models import load_model\n",
        "\n",
        "# Load model\n",
        "model = load_model('F:\\\\Niranjan\\\\model.h5')\n",
        "\n",
        "# Evaluate train data\n",
        "test_loss, test_accuracy = model.evaluate(train_generator)\n",
        "print(f\"Train Loss: {test_loss}\")\n",
        "print(f\"Train Accuracy: {test_accuracy}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "def preprocess_frame(frame_path):\n",
        "    try:\n",
        "        img = image.load_img(frame_path, target_size=(224, 224))\n",
        "        img_array = image.img_to_array(img)\n",
        "        img_array = np.expand_dims(img_array, axis=0)\n",
        "        return img_array\n",
        "    except PermissionError as e:\n",
        "        print(f\"Permission denied: {e}\")\n",
        "        print(f\"Current working directory: {os.getcwd()}\")\n",
        "        print(f\"File path: {frame_path}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ],
      "source": [
        "# Load the trained model (replace 'model.h5' with your model's path)\n",
        "model = load_model('F:\\\\Niranjan\\\\model.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 912ms/step\n",
            "Prediction for the input video: 0.4999999403953552\n",
            "The video is classified as REAL.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Function to extract frames from video\n",
        "def extract_frames(video_path, frame_rate=1):\n",
        "    frames = []\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    interval = int(fps / frame_rate)  # Extract one frame per second by default\n",
        "    \n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        if int(cap.get(cv2.CAP_PROP_POS_FRAMES)) % interval == 0:\n",
        "            frame = cv2.resize(frame, (224, 224))  # Resizing frames to 224x224 for ResNet50 input\n",
        "            frames.append(frame)\n",
        "    \n",
        "    cap.release()\n",
        "    return np.array(frames)\n",
        "\n",
        "# Function to preprocess frames for ResNet50\n",
        "def preprocess_frames(frames):\n",
        "    frames = frames.astype('float32') / 255.0  # Normalize to [0, 1]\n",
        "    return frames\n",
        "\n",
        "# Function to predict whether the video is real or fake\n",
        "def predict_deepfake(frames, model):\n",
        "    predictions = model.predict(frames)\n",
        "    average_prediction = np.mean(predictions)  # Averaging predictions across frames\n",
        "    return average_prediction\n",
        "\n",
        "# Load the trained ResNet50 model\n",
        "model = load_model('F:\\\\Niranjan\\\\model.h5')\n",
        "\n",
        "# Step 1: User inputs the video file path\n",
        "video_path = input(\"Please enter the path to the video file (either real or fake): \")\n",
        "\n",
        "# Step 2: Extract and preprocess frames from the user's video\n",
        "video_frames = extract_frames(video_path)\n",
        "video_frames = preprocess_frames(video_frames)\n",
        "\n",
        "# Step 3: Predict the video's authenticity (real or fake)\n",
        "video_prediction = predict_deepfake(video_frames, model)\n",
        "\n",
        "# Step 4: Print the prediction result\n",
        "print(f'Prediction for the input video: {video_prediction}')\n",
        "\n",
        "# Step 5: Interpretation of the result (assuming a threshold of 0.5)\n",
        "threshold = 0.5\n",
        "if video_prediction < threshold:\n",
        "    print(\"The video is classified as REAL.\")\n",
        "else:\n",
        "    print(\"The video is classified as FAKE.\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
